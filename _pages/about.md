---
permalink: /
title: "Yachao Zhang(Âº†‰∫öË∂Ö)"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
üåà I am an Assistant Professor at School of Informatics, Xiamen University in Xiamen, Fujian, China.  Previously, I worked as a postdoctoral researcher at the Intelligent Computing Lab, SIGS of Tsinghua University, working with Prof. Xiu Li. 

üè≥Ô∏è‚Äçüåà My research interests include, but are not limited to: **3D Computer Vision** (3D computer vision, 3D digital human avatar), **Machine Learning** (weakly-supervised learning, unsupervised learning, transfer learning). If you are interested in my research or have any use cases that you want to share, feel free to contact me!

ü§°**I am actively recruiting self-motivated master students.** ü§ù

<li> I plan to admit 2-4 master students in 2025 and 2026. I am not concerned about the level of your undergraduate institution; I require a down-to-earth attitude, the drive to improve, a strong sense of self-motivation, and compliance with the laboratory‚Äôs unified management.

<li> For master students who aspire to work in the industry, I will focus on cultivating their practical skills. 

<li> For master students who wish to further their academic studies, I will train them to the standard of PhD students, equipping them with the ability to conduct independent research.  

<li> I am also recruiting several high-year undergraduate interns. The recruitment is aimed at students majoring in computer science, automation, mathematics, and related fields.
_______________________________________________________________________________________________________

<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>NewsÊªöÂä®Â±ïÁ§∫</title>
<style>
  .scroll-container {
    max-height: 500px; /* ËÆæÁΩÆÊúÄÂ§ßÈ´òÂ∫¶ */
    overflow-y: auto; /* Ê∑ªÂä†ÂûÇÁõ¥ÊªöÂä®Êù° */
    border: 1px solid #ccc; /* ÂèØÈÄâÔºöÊ∑ªÂä†ËæπÊ°Ü */
    padding: 10px; /* ÂèØÈÄâÔºöÊ∑ªÂä†ÂÜÖËæπË∑ù */
  }
  .mini ul {
    list-style-type: none;
    padding: 0;
  }
  .mini li {
    margin-bottom: 5px; /* Ê∑ªÂä†ÂàóË°®È°π‰πãÈó¥ÁöÑÈó¥Ë∑ù */
  }
</style>
</head>
<body>
<h3>
  <a name="news"></a> ‚úçNews
</h3>
<div class="scroll-container">
  <div class="mini">
    <ul>
  <li> <strong>[Feb 2025]</strong> One paper about Text-to-Motion Generation with GPT-4Vision Reward is accepted by CVPR 2025!</li>
  <li> <strong>[Feb 2025]</strong> One paper about cross-modal 3D semantic segmentation is accepted by TCSVT!</li>
  <li> <strong>[Jan 2025]</strong> One paper about weakly-supervised point cloud semantic segmentation is accepted by TNNLS!</li>
  <li> <strong>[Dec 2024]</strong> One paper about multi-modal learning are accepted by AAAI 2025!</li>
  <li> <strong>[Sep 2024]</strong> Three papers about multi-modal learning are accepted by NeurIPS 2024!</li>
  <li> <strong>[Aug 2024]</strong> Three papers about multi-modal learning are accepted by ACMMM 2024!</li>
  <li> <strong>[Jul 2024]</strong> One paper about multi-modal label efficient learning is accepted by ECCV 2024!</li>
  <li> <strong>[Jun 2024]</strong> One paper about Object Detection is accepted by TNNLS!</li>
  <li> <strong>[Feb 2024]</strong> One paper about AI ChoreoMaster is accepted by CVPR 2024!</li>
  <li> <strong>[Jan 2024]</strong> One papers about about camouflaged object detection is accepted by ICLR 2024!</li>
  <li> <strong>[Dec 2023]</strong> Three papers about digital human avatar and cross-modal zero-shot learning are accepted by ICASSP 2024!</li>
  <li> <strong>[Dec 2023]</strong> Two papers about multi-modal understanding and multi-modal gestures generation are accepted by AAAI 2024!</li>
  <li> <strong>[Oct 2023]</strong> Our paper about Semi-Supervised Defect Segmentation is accepted by TNNLS!</li>
  <li> <strong>[Sep 2023]</strong> One paper about weakly supervised learning is accepted by NeurIPS 2023!</li>
  <li> <strong>[Aug 2023]</strong> My application of the National Natural Science Foundation of China is approved!</li>
  <li> <strong>[Jul 2023]</strong> One paper about multi-modal unsupervised domain adaptation semantic segmentation is accepted by ACMMM 2023!</li>
  <li> <strong>[Jul 2023]</strong> Four papers about multi-modal learning, ANN2SNN, and  <a href="https://li-ronghui.github.io/finedance.html">AI ChoreoMaster</a> are accepted by ICCV 2023!</li>
  <li> <strong>[Jun 2023]</strong> My application of the China Postdoctoral Science Foundation is approved!</li>
  <li> <strong>[Apr 2023]</strong> One paper about zero-shot learning is accepted by IJCAI 2023!</li>
  <li> <strong>[Mar 2023]</strong> One paper about camouflaged object detection is accepted by CVPR 2023!</li>
  <li> <strong>[Feb 2023]</strong> One paper about long-tailed learning is accepted by ICASSP 2023!</li>
  <li> <strong>[Nov 2022]</strong> One paper about weakly-supervised point cloud semantic segmentation is accepted by AAAI 2023!</li>
  <li> <strong>[Aug 2022]</strong> One paper about binary multi-view cluster is accepted by TNNLS!</li>
  <li> <strong>[Jul 2022]</strong> Two papers about multi-modal unsupervised domain adaptation semantic segmentation are accepted by ACMMM 2022!</li>
  <li> <strong>[Jun 2022]</strong> I am the winner of the title of "Outstanding Graduate" of Xiamen University!</li>
  <li> <strong>[Jun 2022]</strong> I successfully defended my PhD thesis!</li>
  <li> <strong>[Mar 2022]</strong> I am the winner of Xiamen University Liu Yubin Youth Science and Technology Scholarship!</li>
  <li> <strong>[Nov 2021]</strong> I am the winner of the title of "Outstanding Student" of Xiamen University!</li>
  <li> <strong>[Dec 2021]</strong> I am the winner of the first prize of the 2021 Excellent Paper Award of Fujian Computer Society!</li>
  <li> <strong>[Jul 2021]</strong> One paper about weakly-supervised point cloud  semantic segmentation is accepted by ICCV 2021!</li>
  <li> <strong>[Mar 2021]</strong> One paper about PolSAR Image Classification is accepted by Remote Sensing!</li>
  <li> <strong>[Dec 2020]</strong> One paper about weakly-supervised large scale point cloud semantic segmentation is accepted by AAAI 2021!</li>
    </ul>
  </div>
</div>

</body>
</html>

_______________________________________________________________________________________________________

<h3>
  <a name="Publications"></a> üìöSelected Publications (1Ô∏è‚É£ Equal contribution, üìß Corresponding author)
</h3>
<font face="helvetica, ariel, &#39;sans serif&#39;">
        <table cellspacing="0" cellpadding="0" class="noBorder">
           <tbody>
               <tr>
                    <td width="40%">
                        <img width="320" src="../images/t2m.png" border="0">
                            </td>
                    <td>
                            <b>AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward</b>
                    <br>
                    Haonan Han, Xiangzuo Wu, Huan Liao, Zunnan Xu, Zhongyuan Hu, Ronghui Li, <strong>Yachao Zhang</strong>üìß, Xiu Liüìß. 
                    <br>
                    <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR, 2025)</em>
                    <br>
                   [<a href="https://arxiv.org/abs/2411.18654">Paper</a>][<a href="https://atom-motion.github.io/">Project Page</a>]
                    </td>
               </tr>
               <tr>
                    <td width="40%">
                        <img width="320" src="../images/C3.png" border="0">
                            </td>
                    <td>
                            <b>Cross-Cloud Consistency for Weakly Supervised Point Cloud Semantic Segmentation</b>
                    <br>
                    <strong>Yachao Zhang</strong>, Yuxiang Lan, Yuan Xie, Cuihua Li, Yanyun Quüìß. 
                    <br>
                    <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS, 2025)</em>
                    <br>
                   [<a href="https://ieeexplore.ieee.org/abstract/document/10843141">Paper</a>][<a href="https://github.com/Yachao-Zhang/Cross-Cloud-Consistency">Code</a>]
                    </td>
               </tr>
                    <tr>
                    <td width="40%">
                        <img width="320" src="../images/nips24.png" border="0">
                            </td>
                    <td>
                            <b>UniDSeg: Unified Cross-Domain 3D Semantic Segmentation via Visual Foundation Models Prior</b>
                    <br>
                     Yao Wu, Mingwei Xing,  <strong>Yachao Zhang</strong>üìß, Xiaotong Luo, Yuan Xie, Yanyun Quüìß
                    <br>
                    <em>Annual Conference on Neural Information Processing Systems (NeurIPS 2024)</em>
                    <br>
                   [<a href="https://neurips.cc/virtual/2024/poster/94354">Paper</a>][<a href="https://anonymous.4open.science/r/UniDSeg-4BC1/">Code</a>]
                    </td>
               </tr>
               <tr>
                    <td width="40%">
                        <img width="320" src="../images/mabbatalk.jpg" border="0">
                            </td>
                    <td>
                            <b>MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models</b>
                    <br>
                     Zunnan Xu, Yukang LinÔºåHaonan Han, Sicheng Yang, Ronghui Li, <strong>Yachao Zhang</strong>üìßÔºåXiu Liüìß. 
                    <br>
                    <em>Annual Conference on Neural Information Processing Systems (NeurIPS 2024)</em>
                    <br>
                   [<a href="https://arxiv.org/pdf/2403.09471">Paper</a>][<a href="https://github.com/kkakkkka/MambaTalk">Code</a>][<a href="https://kkakkkka.github.io/MambaTalk/">Project</a>]
                    </td>
               </tr>
            <tr>
                    <td width="40%">
                        <img width="220" src="../images/4.png" border="0">
                            </td>
                    <td>
                            <b>Consistent123: One image to highly consistent 3d asset using case-aware diffusion priors</b>
                    <br>
                     Yukang LinÔºåHaonan Han, Chaoqun Gong, Zunnan Xu, <strong>Yachao Zhang</strong>üìßÔºåXiu Liüìß.
                    <br>
                    <em>ACM International Conference on Multimedia (ACMMM 2024)</em>
                    <br>
                   [<a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=a-I8c8EAAAAJ&citation_for_view=a-I8c8EAAAAJ:0EnyYjriUFMC">Paper</a>][<a href="https://consistent123.github.io/">Project</a>]
                    </td>
               </tr>
           <tr>
                    <td width="40%">
                        <img width="320" src="../images/gesture.jpg" border="0">
                            </td>
                    <td>
                            <b>Chain of Generation: Multi-Modal Gesture Synthesis via Cascaded Conditional Control</b>
                    <br>
                     Zunnan XuÔºå<strong>Yachao Zhang</strong>üìßÔºåSicheng YangÔºåRonghui LiÔºåXiu Liüìß.
                    <br>
                    <em>Association for the Advance of Artificial Intelligence (AAAI 2024)</em>
                    <br>
                   [<a href="https://arxiv.org/abs/2312.15900">Paper</a>][<a href="https://github.com/Yachao-Zhang">Code coming soon!</a>]
                    </td>
               </tr>
             <tr>
                    <td width="40%">
                        <img width="320" src="../images/xmatch.jpg" border="0">
                            </td>
                    <td>
                            <b>Cross-Modal Match for Language Conditioned 3D Object Grounding</b>
                    <br>
                    <strong>Yachao Zhang</strong>, Runze Hu, Ronghui Li, Yanyun Qu, Yuan Xie, Xiu Liüìß.
                    <br>
                    <em>Association for the Advance of Artificial Intelligence (AAAI 2024)</em>
                    <br>
                    [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28566">Paper</a>][<a href="https://github.com/Yachao-Zhang">Code coming soon!</a>]
                    </td>
               </tr>
         <tr>
                    <td width="40%">
                        <img width="320" src="../images/dist.jpg" border="0">
                            </td>
                    <td>
                            <b>Dual Pseudo-Labels Interactive Self-Training for Semi-Supervised Visible-Infrared Person Re-Identification</b>
                    <br>
                     Jiangming Shi<strong>1Ô∏è‚É£</strong>, <strong>Yachao Zhang1Ô∏è‚É£</strong>, Xiangbo Yin, Yuan Xie, Zhizhong Zhang, Jianping Fan, zhongchao shi, Yanyun Qu.
                    <br>
                    <em>IEEE/CVF International Conference on Computer Vision (ICCV 2023)</em>
                    <br>
                   [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Dual_Pseudo-Labels_Interactive_Self-Training_for_Semi-Supervised_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.pdf">Paper</a>][<a href="https://github.com/XiangboYin/DPIS_USVLReID">Code</a>]
                    </td>
               </tr>
             <tr>
                    <td width="40%">
                        <img width="320" src="../images/ann2snn.jpg" border="0">
                            </td>
                    <td>
                            <b>Efficient Converted Spiking Neural Network for 3D and 2D Classification</b>
                    <br>
                    Yuxiang Lan, <strong>Yachao Zhang</strong> üìß, Xu Ma, Yanyun Qu, Yun Fu.
                    <br>
                    <em>IEEE/CVF International Conference on Computer Vision (ICCV 2023)</em>
                    <br>
                    [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lan_Efficient_Converted_Spiking_Neural_Network_for_3D_and_2D_Classification_ICCV_2023_paper.pdf">Paper</a>]
                    </td>
               </tr>
              <tr>
                    <td width="40%">
                        <img width="320" src="../images/BEV-DG.png" border="0">
                            </td>
                    <td>
                      <b>BEV-DG: Cross-Modal Learning under Bird‚Äôs-Eye View for Domain Generalization of 3D Semantic Segmentation</b>
                    <br>
                    Miaoyu Li, <strong>Yachao Zhang</strong> üìß, Xu Ma, Yanyun Qu, Yun Fu.
                    <br>
                    <em>IEEE/CVF International Conference on Computer Vision (ICCV 2023)</em>
                    <br>
                   [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_BEV-DG_Cross-Modal_Learning_under_Birds-Eye_View_for_Domain_Generalization_of_ICCV_2023_paper.pdf">Paper</a>]
                    </td>
               </tr>
          <tr>
                    <td width="40%">
                        <img width="320" src="../images/DoubleConsistency.png" border="0">
                            </td>
                    <td>
                            <b>Weakly Supervised 3D Segmentation via Receptive-driven Pseudo Label Consistency and Structural Consistency</b>
                    <br>
                    Yuxiang Lan<strong>1Ô∏è‚É£</strong>, <strong>Yachao Zhang1Ô∏è‚É£</strong>, Yanyun Qu, Cong Wang, Yuan Xie, Zongze Wu. 
                    <br>
                    <em>Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI 2023)</em>
                    <br>
                    [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/25205">Paper</a>][<a href="https://github.com/Yachao-Zhang/DoubleConsistency">Code</a>]
                    </td>  
               </tr>
                   <tr>
                    <td width="40%">
                        <img width="320" src="../images/vsboost.jpg" border="0">
                            </td>
                    <td>
                            <b>VS-Boost: Boosting Visual-Semantic Association for Generalized Zero-Shot Learning</b>
                    <br>
                    Xiaofan Li, <strong>Yachao Zhang</strong> üìß, Shiran Bian, Yanyun Qu, Yuan Xie, Zhongchao Shi, Jianping Fan.
                    <br>
                    <em>International Joint Conference on Artificial Intelligence (IJCAI 2023)</em>
                    <br>
                   [<a href="https://www.ijcai.org/proceedings/2023/0123.pdf">Paper</a>]
                    </td>
               </tr>
               <tr>
                    <td width="40%">
                        <img width="320" src="../images/dual-cross.jpg" border="0">
                            </td>
                    <td>
                            <b>Cross-Domain and Cross-Modal Knowledge Distillation in Domain Adaptation for 3D Semantic Segmentation</b>
                    <br>
                    Miaoyu Li<strong>1Ô∏è‚É£</strong>,<strong>Yachao Zhang1Ô∏è‚É£</strong>,Miaoyu Li, Yuan Xie, Zhizhong Zhang, Cuihua Li, Yanyun Qu. 
                    <br>
                    <em>ACM International Conference on Multimedia (ACMMM 2022)</em>
                    <br>
                   [<a href="https://dl.acm.org/doi/10.1145/3503161.3547990">Paper</a>][<a href="https://github.com/Yachao-Zhang/Dual-Cross">Code</a>]
                    </td>
               </tr>
                    <tr>
                    <td width="40%">
                        <img width="320" src="../images/all-in.png" border="0">
                            </td>
                    <td>
                            <b>Learning All-In Collaborative Multiview Binary Representation for Clustering</b>
                    <br>
                    <strong>Yachao Zhang</strong>, Yuan Xie, Zongze Wu, Cuihua Li, Yanyun Qu. 
                    <br>
                    <em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS 2022)</em>
                    <br>
                    [<a href="https://ieeexplore.ieee.org/document/9882008/">Paper</a>][<a href="https://github.com/Yachao-Zhang/All_In_Learning">Code</a>]
                    </td>
               </tr>  
                    <tr>
                    <td width="40%">
                        <img width="320" src="../images/SSE-xMUDA.jpg" border="0">
                            </td>
                    <td>
                    <b>SSE-xMUDA: Self-supervised Exclusive Learning for 3D Segmentation in Cross-Modal Unsupervised Domain Adaptation </b>
                    <br>
                    <strong>Yachao Zhang</strong>,Miaoyu Li, Yuan Xie, Zhizhong Zhang, Cuihua Li, Yanyun Qu.
                    <br>
                    <em>ACM International Conference on Multimedia (ACMMM 2022)</em>
                    <br>
                    [<a href="https://doi.org/10.1145/3503161.3547987">Paper</a>][<a href="https://github.com/Yachao-Zhang/SSE-xMUDA">Code</a>]
                    </td>
                </tr>  
            <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/PSD.jpg" border="0">
                            </td>
                    <td>
                      <b>Perturbed Self-Distillation: Weakly Supervised Large-Scale Point Cloud Semantic Segmentation </b>
                      <br>
                      <strong>Yachao Zhang</strong>, Yanyun Qu, Zhonghao Li, Shanshan Zheng, Cuihua Li. 
                      <br>
                      <em>IEEE Conference on International Conference on Computer Vision (ICCV 2021)</em>
                      <br>
                      [<a href="https://openaccess.thecvf.com/content/ICCV2021/html/Zhang_Perturbed_Self-Distillation_Weakly_Supervised_Large-Scale_Point_Cloud_Semantic_Segmentation_ICCV_2021_paper.html">Paper</a>][<a href="https://github.com/Yachao-Zhang/PSD">Code</a>]
              </td>
           </tr>
           <tr>
                    <td class="noBorder" width="40%">
                        <img width="320" src="../images/WS3.jpg" border="0">
                            </td>
                    <td>
                    <b>Weakly supervised semantic segmentation for large-scale point cloud </b>
                    <br>
                    <strong>Yachao Zhang</strong>, Zonghao Li, Yuan Xie, Yanyun Qu, Cuihua Li, Tao Mei. 
                    <br>
                    <em>Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI 2021)</em>
                    <br>
                    [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/16455">Paper</a>][<a href="https://github.com/Yachao-Zhang/WS3">Code</a>]
                    </td>
             </tr>
           </tbody>
           </table>
</font>
[Please visit [my google scholar profile](https://scholar.google.com/citations?user=a-I8c8EAAAAJ&hl=en) for the full publication list.]
_______________________________________________________________________________________________________

<h3>
  <a name="services"></a> üì†Academic Services
</h3>
<div class="mini">
  <ul>
  <li> <strong>Conference Reviewer</strong>: CVPR, ICML, NeurIPS, AAAI, ICCV, ACMMM, ICLR </li>
  <li> <strong>Journal Reviewer</strong>: IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Intelligent Transportation Systems, IEEE Transactions on Artificial Intelligence, IEEE Transactions on Image Processing</li>
  </ul>
</div>
 
_______________________________________________________________________________________________________

<h3>
  <a name="services"></a> ‚ú®Hobby
</h3>
<div class="mini">
 <td width="30%">
 <img width="60" src="../images/ball.jpg" border="0">
</td>
   <td width="30%">
 <img width="60" src="../images/reading.jpg" border="0">
</td>
   <td width="30%">
 <img width="60" src="../images/riding.jpg" border="0">
</td>
   <td width="30%">
 <img width="60" src="../images/tubu.jpg" border="0">
</td>
</div>

 
_______________________________________________________________________________________________________

<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=ve6F6SS11iG4uIguoTWVRUjvilkuBNsM2hxvFs-6aos&cl=ffffff&w=a"></script>
